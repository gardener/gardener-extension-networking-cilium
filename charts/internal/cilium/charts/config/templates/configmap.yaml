{{- /*  Default values with backwards compatibility */ -}}
{{- $defaultBpfMapDynamicSizeRatio := 0.0 -}}
{{- $defaultBpfClockProbe := "false" -}}
{{- $defaultSessionAffinity := "false" -}}
{{- $defaultOperatorApiServeAddr := "localhost:9234" -}}
{{- $defaultBpfCtTcpMax := 524288 -}}
{{- $defaultBpfCtAnyMax := 262144 -}}
{{- $enableIdentityMark := "true" -}}

{{- /* Default values when 1.8 was initially deployed */ -}}
{{- if semverCompare ">=1.8" (default "1.8" .Values.upgradeCompatibility) -}}
{{- $defaultBpfMapDynamicSizeRatio = 0.0025 -}}
{{- $defaultBpfClockProbe := "false" -}}
{{- $defaultSessionAffinity = "true" -}}
{{- if .Values.global.ipv4.enabled }}
{{- $defaultOperatorApiServeAddr = "127.0.0.1:9234" -}}
{{- else -}}
{{- $defaultOperatorApiServeAddr = "[::1]:9234" -}}
{{- end }}
{{- $defaultBpfCtTcpMax = 0 -}}
{{- $defaultBpfCtAnyMax = 0 -}}
{{- end -}}

{{- $bpfCtTcpMax := (coalesce .Values.global.bpf.ctTcpMax $defaultBpfCtTcpMax) -}}
{{- $bpfCtAnyMax := (coalesce .Values.global.bpf.ctAnyMax $defaultBpfCtAnyMax) -}}

apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: {{ .Release.Namespace }}
data:

  # Identity allocation mode selects how identities are shared between cilium
  # nodes by setting how they are stored. The options are "crd", "kvstore" or
  # "doublewrite-readkvstore" / "doublewrite-readcrd".
  # - "crd" stores identities in kubernetes as CRDs (custom resource definition).
  #   These can be queried with:
  #     kubectl get ciliumid
  # - "kvstore" stores identities in an etcd kvstore, that is
  #   configured below. Cilium versions before 1.6 supported only the kvstore
  #   backend. Upgrades from these older cilium versions should continue using
  #   the kvstore by commenting out the identity-allocation-mode below, or
  #   setting it to "kvstore".
  # - "doublewrite" modes store identities in both the kvstore and CRDs. This is useful
  #   for seamless migrations from the kvstore mode to the crd mode. Consult the
  #   documentation for more information on how to perform the migration.
  identity-allocation-mode: {{ .Values.global.identityAllocationMode }}
{{- if .Values.global.identityHeartbeatTimeout }}
  identity-heartbeat-timeout: "{{ .Values.global.identityHeartbeatTimeout }}"
{{- end }}
{{- if .Values.global.identityGCInterval }}
  identity-gc-interval: "{{ .Values.global.identityGCInterval }}"
{{- end }}
{{- if .Values.global.endpointGCInterval }}
  cilium-endpoint-gc-interval: "{{ .Values.global.endpointGCInterval }}"
{{- end }}
{{- if .Values.global.nodesGCInterval }}
  nodes-gc-interval: "5m0s"
{{- end }}
{{- if .Values.global.disableEndpointCrd }}
  disable-endpoint-crd: "{{ .Values.global.disableEndpointCrd }}"
{{- end }}

{{- if .Values.identityChangeGracePeriod }}
  # identity-change-grace-period is the grace period that needs to pass
  # before an endpoint that has changed its identity will start using
  # that new identity. During the grace period, the new identity has
  # already been allocated and other nodes in the cluster have a chance
  # to whitelist the new upcoming identity of the endpoint.
  identity-change-grace-period: {{ default "5s" .Values.identityChangeGracePeriod | quote }}
{{- end }}

  # If you want to run cilium in debug mode change this value to true
  debug: {{ .Values.global.debug.enabled | quote }}
  # The agent can be put into the following three policy enforcement modes
  # default, always and never.
  # https://docs.cilium.io/en/latest/policy/intro/#policy-enforcement-modes
  enable-policy: "{{ .Values.global.agent.policyMode }}"

  policy-cidr-match-mode: "nodes"

{{- if .Values.global.debug.verbose }}
  debug-verbose: "{{ .Values.global.debug.verbose }}"
{{- end }}

{{- if ne (int .Values.global.agent.healthPort) 9876 }}
  # Set the TCP port for the agent health status API. This is not the port used
  # for cilium-health.
  agent-health-port: "{{ .Values.global.agent.healthPort }}"
{{- end }}

{{- if .Values.global.prometheus.enabled }}
  # If you want metrics enabled in all of your Cilium agents, set the port for
  # which the Cilium agents will have their metrics exposed.
  # This option deprecates the "prometheus-serve-addr" in the
  # "cilium-metrics-config" ConfigMap
  # NOTE that this will open the port on ALL nodes where Cilium pods are
  # scheduled.
  prometheus-serve-addr: ":{{ .Values.global.prometheus.port }}"
{{- end }}

  # A space-separated list of controller groups for which to enable metrics.
  # The special values of "all" and "none" are supported.
  controller-group-metrics:
    write-cni-file
    sync-host-ips
    sync-lb-maps-with-k8s-services


{{- if .Values.global.operatorPrometheus.enabled }}
  # If you want metrics enabled in cilium-operator, set the port for
  # which the Cilium Operator will have their metrics exposed.
  # NOTE that this will open the port on the nodes where Cilium operator pod
  # is scheduled.
  operator-prometheus-serve-addr: ":{{ .Values.global.operatorPrometheus.port }}"
  enable-metrics: "true"
{{- end }}

  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4
  # address.
{{- if .Values.global.ipv4 }}
  enable-ipv4: {{ .Values.global.ipv4.enabled | quote }}
{{- end }}

  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6
  # address.
{{- if .Values.global.ipv6 }}
  enable-ipv6: {{ .Values.global.ipv6.enabled | quote }}
{{- end }}

{{- if .Values.global.cleanState }}
  # If a serious issue occurs during Cilium startup, this
  # invasive option may be set to true to remove all persistent
  # state. Endpoints will not be restored using knowledge from a
  # prior Cilium run, so they may receive new IP addresses upon
  # restart. This also triggers clean-cilium-bpf-state.
  clean-cilium-state: "true"
{{- end }}

{{- if .Values.global.cleanBpfState }}
  # If you want to clean cilium BPF state, set this to true;
  # Removes all BPF maps from the filesystem. Upon restart,
  # endpoints are restored with the same IP addresses, however
  # any ongoing connections may be disrupted briefly.
  # Loadbalancing decisions will be reset, so any ongoing
  # connections via a service may be loadbalanced to a different
  # backend after restart.
  clean-cilium-bpf-state: "true"
{{- end }}

{{- if .Values.global.cni.customConf }}
  # Users who wish to specify their own custom CNI configuration file must set
  # custom-cni-conf to "true", otherwise Cilium may overwrite the configuration.
  custom-cni-conf: "{{ .Values.global.cni.customConf }}"
{{- end }}

{{- if hasKey .Values "bpfClockProbe" }}
  enable-bpf-clock-probe: {{ .Values.bpfClockProbe | quote }}
{{- else if eq $defaultBpfClockProbe "true" }}
  enable-bpf-clock-probe: {{ $defaultBpfClockProbe | quote }}
{{- end }}

  # If you want cilium monitor to aggregate tracing for packets, set this level
  # to "low", "medium", or "maximum". The higher the level, the less packets
  # that will be seen in monitor output.
  monitor-aggregation: {{ .Values.global.bpf.monitorAggregation }}

  # The monitor aggregation interval governs the typical time between monitor
  # notification events for each allowed connection.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-interval: {{ .Values.global.bpf.monitorInterval }}

  # The monitor aggregation flags determine which TCP flags which, upon the
  # first observation, cause monitor notifications to be generated.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-flags: {{ .Values.global.bpf.monitorFlags }}

{{- if or $bpfCtTcpMax $bpfCtAnyMax }}
  # bpf-ct-global-*-max specifies the maximum number of connections
  # supported across all endpoints, split by protocol: tcp or other. One pair
  # of maps uses these values for IPv4 connections, and another pair of maps
  # use these values for IPv6 connections.
  #
  # If these values are modified, then during the next Cilium startup the
  # tracking of ongoing connections may be disrupted. This may lead to brief
  # policy drops or a change in loadbalancing decisions for a connection.
  #
  # For users upgrading from Cilium 1.2 or earlier, to minimize disruption
  # during the upgrade process, set bpf-ct-global-tcp-max to 1000000.
{{- if $bpfCtTcpMax }}
  bpf-ct-global-tcp-max: {{ $bpfCtTcpMax | quote }}
{{- end }}
{{- if $bpfCtAnyMax }}
  bpf-ct-global-any-max: {{ $bpfCtAnyMax | quote }}
{{- end }}
{{- end }}

{{- if .Values.global.bpf.natMax }}
  # bpf-nat-global-max specified the maximum number of entries in the
  # BPF NAT table.
  bpf-nat-global-max: "{{ .Values.global.bpf.natMax }}"
{{- end }}

{{- if .Values.global.bpf.neighMax }}
  # bpf-neigh-global-max specified the maximum number of entries in the
  # BPF neighbor table.
  bpf-neigh-global-max: "{{ .Values.global.bpf.neighMax }}"
{{- end }}
{{- if hasKey .Values "bpfMapDynamicSizeRatio" }}
  bpf-map-dynamic-size-ratio: {{ .Values.bpfMapDynamicSizeRatio | quote }}
{{- else if ne $defaultBpfMapDynamicSizeRatio 0.0 }}
  # Specifies the ratio (0.0-1.0) of total system memory to use for dynamic
  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.
  bpf-map-dynamic-size-ratio: {{ $defaultBpfMapDynamicSizeRatio | quote }}
{{- end }}
{{- if .Values.global.bpf.policyMapMax }}
  # bpf-policy-map-max specifies the maximum number of entries in endpoint
  # policy map (per endpoint)
  bpf-policy-map-max: "{{ .Values.global.bpf.policyMapMax }}"
{{- end }}
{{- if .Values.global.bpf.lbMapMax }}
  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,
  # backend and affinity maps.
  bpf-lb-map-max: "{{ .Values.global.bpf.lbMapMax }}"
{{- end }}
{{- if .Values.global.bpf.lbMode }}
  {{- if and (eq .Values.global.bpf.lbMode "dsr") (not .Values.global.ipv4NativeRoutingCIDR)  }}
    {{ fail "bpf-lb-mode 'dsr' requires native routing to be enabled" }}
  {{- end }}
  # Specifies the bpf load balancing mode ("snat", "dsr", "hybrid")
  bpf-lb-mode: {{ .Values.global.bpf.lbMode }}
{{- end }}
{{- if .Values.global.bpf.lbExternalClusterip }}
  # bpf-lb-bypass-fib-lookup instructs Cilium to enable the FIB lookup bypass
  # optimization for nodeport reverse NAT handling.
  bpf-lb-external-clusterip: "{{ .Values.global.bpf.lbExternalClusterip }}"
{{- end }}

{{- if .Values.global.bpf.lbSourceRangeAllTypes }}
  bpf-lb-source-range-all-types: {{ .Values.global.bpf.lbSourceRangeAllTypes | quote }}
{{- end }}
{{- if .Values.global.bpf.lbAlgorithmAnnotation }}
  bpf-lb-algorithm-annotation: {{ .Values.global.bpf.lbAlgorithmAnnotation | quote }}
{{- end }}
{{- if .Values.global.bpf.lbModeAnnotation }}
  bpf-lb-mode-annotation: {{ .Values.global.bpf.lbModeAnnotation | quote }}
{{- end }}

  bpf-distributed-lru: "false"
  bpf-events-drop-enabled: "true"
  bpf-events-policy-verdict-enabled: "true"
  bpf-events-trace-enabled: "true"

  # Enable socket-based LB for E/W traffic
{{- if or (eq .Values.global.kubeProxyReplacement "partial") (eq .Values.global.kubeProxyReplacement "false") }}
  bpf-lb-sock: "true"
{{- else}}
  bpf-lb-sock:  "{{ .Values.global.bpfSocketLB.enabled }}"
{{- end }}

{{- if .Values.global.bpfSocketLBHostnsOnly.enabled }}
  # bpf-lb-sock-hostns-only skip socket LB for services when inside a pod namespace, in favor of service LB at the pod interface.
  # Socket LB is still used when in the host namespace. Required by service mesh (e.g., Istio, Linkerd).
  bpf-lb-sock-hostns-only: "{{ .Values.global.bpfSocketLBHostnsOnly.enabled }}"
{{- end }}

  # Pre-allocation of map entries allows per-packet latency to be reduced, at
  # the expense of up-front memory allocation for the entries in the maps. The
  # default value below will minimize memory usage in the default installation;
  # users who are sensitive to latency may consider setting this to "true".
  #
  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore
  # this option and behave as though it is set to "true".
  #
  # If this value is modified, then during the next Cilium startup the restore
  # of existing endpoints and tracking of ongoing connections may be disrupted.
  # As a result, reply packets may be dropped and the load-balancing decisions
  # for established connections may change.
  #
  # If this option is set to "false" during an upgrade from 1.3 or earlier to
  # 1.4 or later, then it may cause one-time disruptions during the upgrade.
  preallocate-bpf-maps: "{{ .Values.global.bpf.preallocateMaps }}"

  # Name of the cluster. Only relevant when building a mesh of clusters.
  cluster-name: {{ .Values.global.cluster.name }}

{{- if .Values.global.cluster.id }}
  # Unique ID of the cluster. Must be unique across all conneted clusters and
  # in the range of 1 and 255. Only relevant when building a mesh of clusters.
  cluster-id: "{{ .Values.global.cluster.id }}"
{{- end }}

  # Encapsulation mode for communication between nodes
  # Possible values:
  #   - disabled
  #   - vxlan (default)
  #   - geneve
{{- if eq .Values.global.tunnel "disabled" }}
  routing-mode: "native"
{{- else if eq .Values.global.tunnel "vxlan" }}
  routing-mode: "tunnel"
  tunnel-protocol: "vxlan"
{{- else if eq .Values.global.tunnel "geneve" }}
  routing-mode: "tunnel"
  tunnel-protocol: "geneve"
{{- end }}
  # -- Configure VXLAN and Geneve tunnel source port range hint.
  # @default -- 0-0 to let the kernel driver decide the range
  tunnel-source-port-range: "0-0"
  service-no-backend-response: "reject"

{{- if .Values.global.eni }}
  enable-endpoint-routes: "true"
  auto-create-cilium-node-resource: "true"
  blacklist-conflicting-routes: "false"
{{- end }}

{{- if .Values.global.azure.enabled }}
  enable-endpoint-routes: "true"
  auto-create-cilium-node-resource: "true"
  blacklist-conflicting-routes: "false"
  enable-local-node-route: "false"
{{- end }}

{{- if .Values.global.l7Proxy }}
  # Enables L7 proxy for L7 policy enforcement and visibility
  enable-l7-proxy: {{ .Values.global.l7Proxy.enabled | quote }}
{{- end }}

  enable-ipv4-big-tcp: {{ .Values.global.enableIpv4BigTCP | quote }}
{{- if .Values.global.ipv4.enabled }}
  enable-ipv4-masquerade: {{ .Values.global.enableIpv4Masquerade | quote }}
{{- else }}
  enable-ipv4-masquerade: "false"
{{- end }}
  enable-ipv6-big-tcp: {{ .Values.global.enableIpv6BigTCP | quote }}
  enable-ipv6-masquerade: {{ .Values.global.enableIpv6Masquerade | quote }}
  enable-tcx: "true"
  datapath-mode: "veth"
{{- if and .Values.global.ipv4.enabled (and (not .Values.global.snatOutOfCluster.enabled) (not .Values.global.snatToUpstreamDNS.enabled )) }}
  enable-bpf-masquerade:  {{ .Values.global.enableBPFMasquerade | quote }}
{{- end }}
  enable-masquerade-to-route-source: "false"

  enable-xt-socket-fallback: {{ .Values.global.enableXTSocketFallback | quote }}

{{- if .Values.global.iptablesRandomFully }}
  iptables-random-fully: {{ .Values.global.iptablesRandomFully | quote }}
{{- end }}

  install-iptables-rules: {{ .Values.global.installIptablesRules | quote }}
  install-no-conntrack-iptables-rules: {{ .Values.global.installNoConntrackIptablesRules | quote }}

  auto-direct-node-routes: {{ .Values.global.autoDirectNodeRoutes | quote }}
  direct-routing-skip-unreachable: "false"

{{- if .Values.global.localRedirectPolicy.enabled }}
  enable-local-redirect-policy: {{ .Values.global.localRedirectPolicy.enabled | quote }}
{{- end }}
  enable-runtime-device-detection: "true"

  # DNS Polling periodically issues a DNS lookup for each `matchName` from
  # cilium-agent. The result is used to regenerate endpoint policy.
  # DNS lookups are repeated with an interval of 5 seconds, and are made for
  # A(IPv4) and AAAA(IPv6) addresses. Should a lookup fail, the most recent IP
  # data is used instead. An IP change will trigger a regeneration of the Cilium
  # policy for each endpoint and increment the per cilium-agent policy
  # repository revision.
  #
  # This option is disabled by default starting from version 1.4.x in favor
  # of a more powerful DNS proxy-based implementation, see [0] for details.
  # Enable this option if you want to use FQDN policies but do not want to use
  # the DNS proxy.
  #
  # To ease upgrade, users may opt to set this option to "true".
  # Otherwise please refer to the Upgrade Guide [1] which explains how to
  # prepare policy rules for upgrade.
  #
  # [0] http://docs.cilium.io/en/stable/policy/language/#dns-based
  # [1] http://docs.cilium.io/en/stable/install/upgrade/#changes-that-may-require-action
  tofqdns-enable-poller: "false"

{{- if ne .Values.global.cni.chainingMode "none" }}
  # Enable chaining with another CNI plugin
  #
  # Supported modes:
  #  - none
  #  - aws-cni
  #  - portmap (Enables HostPort support for Cilium)
  cni-chaining-mode: {{ .Values.global.cni.chainingMode }}

{{- if hasKey .Values "enableIdentityMark"}}
  enable-identity-mark: {{ .Values.global.enableIdentityMark | quote }}
{{- else if (ne $enableIdentityMark "true") }}
  enable-identity-mark: "false"
{{- end }}

{{- if ne .Values.global.cni.chainingMode "portmap" }}
  # Disable the PodCIDR route to the cilium_host interface as it is not
  # required. While chaining, it is the responsibility of the underlying plugin
  # to enable routing.
  enable-local-node-route: "false"
{{- end }}
{{- end }}

{{- if .Values.global.egressMasqueradeInterfaces }}
  egress-masquerade-interfaces: {{ .Values.global.egressMasqueradeInterfaces }}
{{- end }}
{{- if and .Values.global.ipMasqAgent .Values.global.ipMasqAgent.enabled }}
  enable-ip-masq-agent: "true"
{{- end }}

{{- if .Values.global.encryption.enabled }}
  enable-ipsec: {{ .Values.global.encryption.enabled | quote }}
  ipsec-key-file: {{ .Values.global.encryption.mountPath }}/{{ .Values.global.encryption.keyFile }}
{{- if .Values.global.encryption.interface }}
  encrypt-interface: {{ .Values.global.encryption.interface }}
{{- end }}
{{- if .Values.global.encryption.nodeEncryption }}
  encrypt-node: {{ .Values.global.encryption.nodeEncryption | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.iptablesLockTimeout }}
  iptables-lock-timeout: {{ .Values.global.iptablesLockTimeout | quote }}
{{- end }}
{{- if .Values.global.nativeRoutingCIDR }}
  native-routing-cidr: {{ .Values.global.nativeRoutingCIDR }}
{{- end }}
{{- if .Values.global.ipv4NativeRoutingCIDR }}
  ipv4-native-routing-cidr: {{ .Values.global.ipv4NativeRoutingCIDR }}
{{- end }}
{{- if .Values.global.ipv6NativeRoutingCIDR }}
  ipv6-native-routing-cidr: {{ .Values.global.ipv6NativeRoutingCIDR }}
{{- end }}

{{- if .Values.global.hostFirewall }}
  enable-host-firewall: {{ .Values.global.hostFirewall | quote }}
{{- end}}

{{- if .Values.global.mtu }}
  mtu: {{ .Values.global.mtu | quote }}
{{- end}}

{{- if .Values.global.devices }}
  # List of devices used to attach bpf_host.o (implements BPF NodePort,
  # host-firewall and BPF masquerading)
  devices: {{ join " " .Values.global.devices | quote }}
{{- end }}

{{- if .Values.global.egressGateway.enabled }}
  {{- if not (or (eq .Values.global.kubeProxyReplacement "strict") (eq .Values.global.kubeProxyReplacement "true")) }}
    {{ fail "kubeProxyReplacement must be set to 'strict' or 'true' in order to enable egressGateway" }}
  {{- end}}
  {{- if eq .Values.global.tunnel "disabled" }}
      {{ fail "cilium must run with an overlay network in order to enable egressGateway" }}
  {{- end}}
  enable-ipv4-egress-gateway: "true"
{{- end}}
{{- if .Values.global.kubeProxyReplacement }}
  kube-proxy-replacement:  {{ .Values.global.kubeProxyReplacement | quote }}
{{- end }}
{{- if ne .Values.global.kubeProxyReplacement "disabled" }}
  kube-proxy-replacement-healthz-bind-address: {{ .Values.global.kubeProxyReplacementHealthzBindAddr | quote}}
{{- end }}
{{- if or (or (eq .Values.global.kubeProxyReplacement "false") (eq .Values.global.kubeProxyReplacement "partial")) (eq .Values.global.kubeProxyReplacement "disabled") }}
  enable-health-check-nodeport: "false"
{{- end }}

{{- if .Values.global.hostServices }}
{{- if .Values.global.hostServices.enabled }}
  enable-host-reachable-services: {{ .Values.global.hostServices.enabled | quote }}
{{- end }}
{{- if ne .Values.global.hostServices.protocols "tcp,udp" }}
  host-reachable-services-protos: {{ .Values.global.hostServices.protocols }}
{{- end }}
{{- end }}
{{- if .Values.global.hostPort }}
{{- if or (eq .Values.global.kubeProxyReplacement "partial") (eq .Values.global.kubeProxyReplacement "false") }}
  enable-host-port: {{ .Values.global.hostPort.enabled | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.externalIPs }}
{{- if or (eq .Values.global.kubeProxyReplacement "partial") (eq .Values.global.kubeProxyReplacement "false") }}
  enable-external-ips: {{ .Values.global.externalIPs.enabled | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.nodePort }}
{{- if or (eq .Values.global.kubeProxyReplacement "partial") (eq .Values.global.kubeProxyReplacement "false") }}
  enable-node-port: "true"
{{- end }}
  nodeport-addresses: ""
{{- if .Values.global.nodePort.range }}
  node-port-range: {{ .Values.global.nodePort.range | quote }}
{{- end }}
{{- if .Values.global.nodePort.device }}
  device: {{ .Values.global.nodePort.device | quote }}
{{- end }}
{{- if .Values.global.nodePort.directRoutingDevice }}
  direct-routing-device: {{ .Values.global.nodePort.directRoutingDevice | quote }}
{{- end }}
{{- if .Values.global.nodePort.mode }}
  node-port-mode: {{ .Values.global.nodePort.mode | quote }}
{{- end }}
{{- if .Values.global.nodePort.acceleration }}
  node-port-acceleration: {{ .Values.global.nodePort.acceleration | quote }}
{{- end }}
  enable-health-check-loadbalancer-ip: "false"
  node-port-bind-protection: {{ .Values.global.nodePort.bindProtection | quote }}
  enable-auto-protect-node-port-range: {{ .Values.global.nodePort.autoProtectPortRange | quote }}
  enable-service-topology: {{ .Values.global.loadBalancer.serviceTopology | quote }}
  enable-svc-source-range-check: {{ .Values.global.enableSvcSrcRangeCheck | quote }}
{{- end }}
{{- if hasKey .Values "sessionAffinity" }}
  enable-session-affinity: {{ .Values.sessionAffinity | quote }}
{{- else if eq $defaultSessionAffinity "true" }}
  enable-session-affinity: {{ $defaultSessionAffinity | quote }}
{{- end }}
{{- if .Values.global.l2NeighDiscovery.enabled }}
  enable-l2-neigh-discovery: {{ .Values.global.l2NeighDiscovery.enabled | quote }}
{{- end }}
{{- if .Values.global.arpingRefreshPeriod }}
  arping-refresh-period: {{ .Values.global.arpingRefreshPeriod }}
{{- end }}
{{- if .Values.global.cni.uninstall }}
  cni-uninstall: {{ .Values.global.cni.uninstall | quote }}
{{- end }}
{{- if .Values.global.enableK8SNetworkpolicy }}
  enable-k8s-networkpolicy: {{ .Values.global.enableK8SNetworkpolicy | quote }}
{{- end }}
  enable-endpoint-lockdown-on-policy-overflow: {{ .Values.global.endpointLockdownOnMapOverflow | quote }}
{{- if and .Values.global.pprof .Values.global.pprof.enabled }}
  pprof: {{ .Values.global.pprof.enabled | quote }}
{{- end }}
{{- if .Values.global.logSystemLoad }}
  log-system-load: {{ .Values.global.logSystemLoad | quote }}
{{- end }}
{{- if .Values.global.logOptions }}
  log-opt: {{ toYaml .Values.global.logOptions | nindent 4 }}
{{- end }}
{{- if and .Values.global.sockops .Values.global.sockops.enabled }}
  sockops-enable: {{ .Values.global.sockops.enabled | quote }}
{{- end }}
{{- if and .Values.global.endpointRoutes .Values.global.endpointRoutes.enabled }}
  enable-endpoint-routes: {{ .Values.global.endpointRoutes.enabled | quote }}
{{- end }}
  write-cni-conf-when-ready: {{ .Values.global.cni.hostConfDirMountPath }}/05-cilium.conflist
  cni-exclusive: {{ .Values.global.cni.exclusive | quote }}
  cni-log-file: {{ .Values.global.cni.logFile }}
{{- if .Values.global.cni.readCniConf }}
  read-cni-conf: {{ .Values.global.cni.readCniConf }}
{{- end }}

{{- if .Values.global.kubeConfigPath }}
  k8s-kubeconfig-path: {{ .Values.global.kubeConfigPath | quote }}
{{- end }}
{{- if and ( .Values.global.endpointHealthChecking.enabled ) (or (eq .Values.global.cni.chainingMode "portmap") (eq .Values.global.cni.chainingMode "none")) }}
  enable-endpoint-health-checking: "true"
{{- else}}
  # Disable health checking, when chaining mode is not set to portmap or none
  enable-endpoint-health-checking: "false"
{{- end }}
{{- if hasKey .Values "healthChecking" }}
  enable-health-checking: {{ .Values.healthChecking | quote }}
{{- end }}
{{- if .Values.global.healthCheckICMPFailureThreshold }}
  health-check-icmp-failure-threshold: {{ .Values.global.healthCheckICMPFailureThreshold | quote }}
{{- end }}
{{- if .Values.global.wellKnownIdentities.enabled }}
  enable-well-known-identities: "true"
{{- else }}
  enable-well-known-identities: "false"
{{- end }}
  enable-node-selector-labels: "false"
  enable-api-rate-limit: {{ .Values.global.apiRateLimit | quote }}
{{- if hasKey .Values "synchronizeK8sNodes" }}
  synchronize-k8s-nodes: {{ .Values.synchronizeK8sNodes | quote }}
{{- end }}
{{- if .Values.global.policyAuditMode }}
  policy-audit-mode: {{ .Values.global.policyAuditMode | quote }}
{{- end }}

{{- if ne $defaultOperatorApiServeAddr "localhost:9234" }}
  operator-api-serve-addr: {{ $defaultOperatorApiServeAddr | quote }}
{{- end }}

{{- if .Values.global.hubble.enabled }}
  enable-hubble: {{ .Values.global.hubble.enabled  | quote }}
  # UNIX domain socket for Hubble server to listen to.
  hubble-socket-path:  {{ .Values.global.hubble.socketPath | quote }}
{{- if .Values.global.hubble.eventQueueSize }}
  # Buffer size of the channel for Hubble to receive monitor events. If this field is not set,
  # the buffer size is set to the default monitor queue size.
  hubble-event-queue-size: {{ .Values.global.hubble.eventQueueSize | quote }}
{{- end }}
{{- if .Values.global.hubble.flowBufferSize }}
  # Size of the buffer to store recent flows.
  hubble-flow-buffer-size: {{ .Values.global.hubble.flowBufferSize | quote }}
{{- end }}
{{- if .Values.global.hubble.metrics.enabled }}
  # Address to expose Hubble metrics (e.g. ":7070"). Metrics server will be disabled if this
  # field is not set.
  hubble-metrics-server: ":{{ .Values.global.hubble.metrics.port }}"
  hubble-metrics-server-enable-tls: "false"
  # A space separated list of metrics to enable. See [0] for available metrics.
  #
  # https://github.com/cilium/hubble/blob/master/Documentation/metrics.md
  hubble-metrics: {{- range .Values.global.hubble.metrics.enabled }}
    {{.}}
  {{- end }}
{{- end }}
  # An additional address for Hubble server to listen to (e.g. ":4244").
  hubble-listen-address: {{ .Values.global.hubble.listenAddress | quote }}
{{- end }}
  hubble-disable-tls: {{ (not .Values.global.hubble.tls.enabled) | quote }}
{{- if .Values.global.hubble.tls.enabled }}
  hubble-tls-auto-enabled: {{ .Values.global.hubble.tls.auto.enabled | quote }}
  hubble-tls-cert-file: {{ .Values.global.hubble.tls.certFile | quote }}
  hubble-tls-key-file: {{ .Values.global.hubble.tls.keyFile | quote }}
  hubble-tls-client-ca-files: {{ .Values.global.hubble.tls.clientCAFiles | quote }}
{{- end }}
{{- if .Values.disableIptablesFeederRules }}
  # A space separated list of iptables chains to disable when installing feeder rules.
  disable-iptables-feeder-rules: {{ .Values.disableIptablesFeederRules | join " " | quote }}
{{- end }}
  ipam: {{ .Values.global.ipam.mode }}
{{- if .Values.global.ipam.multiPoolPreAllocation }}
  ipam-multi-pool-pre-allocation: {{ .Values.global.ipam.multiPoolPreAllocation | quote }}
{{- end }}
{{- if (eq .Values.global.ipam.mode "kubernetes" )}}
  {{- if .Values.global.ipv4.enabled }}
  k8s-require-ipv4-pod-cidr: "true"
  {{- end }}
  {{- if .Values.global.ipv6.enabled }}
  k8s-require-ipv6-pod-cidr: "true"
  {{- end }}
{{- end }}
{{- if (eq .Values.global.ipam.mode "cluster-pool") }}
  ipam-cilium-node-update-rate: "15s"
  {{- if .Values.global.ipv4.enabled }}
  cluster-pool-ipv4-cidr: {{ .Values.global.podCIDR | quote }}
  cluster-pool-ipv4-mask-size: {{ .Values.global.ipam.operator.clusterPoolIPv4MaskSize | quote }}
  {{- end }}
  {{- if .Values.global.ipv6.enabled }}
  cluster-pool-ipv6-cidr: {{ .Values.global.ipam.operator.clusterPoolIPv6PodCIDR | quote }}
  cluster-pool-ipv6-mask-size: {{ .Values.global.ipam.operator.clusterPoolIPv6MaskSize | quote }}
  {{- end }}
{{- end }}

{{- if .Values.global.bgpControlPlane.enabled }}
  enable-bgp-control-plane: "true"
{{- else }}
  enable-bgp-control-plane: "false"
{{- end }}

  egress-gateway-reconciliation-trigger-interval: "1s"
  enable-vtep: "false"
  vtep-endpoint: ""
  vtep-cidr: ""
  vtep-mask: ""
  vtep-mac: ""
  procfs: "/host/proc"
  bpf-root: "/sys/fs/bpf"
  cgroup-root: "/run/cilium/cgroupv2"
  enable-k8s-terminating-endpoint: "true"
  enable-sctp: "false"
  annotate-k8s-node: "true"
  remove-cilium-node-taints: "true"
  set-cilium-node-taints: "true"
  set-cilium-is-up-condition: "true"
  unmanaged-pod-watcher-interval: "15"
  # default DNS proxy to transparent mode in non-chaining modes
  dnsproxy-enable-transparent-mode: "true"
  dnsproxy-socket-linger-timeout: "10"
  tofqdns-dns-reject-response-code: "refused"
  tofqdns-enable-dns-compression: "true"
  tofqdns-endpoint-max-ip-per-hostname: "1000"
  tofqdns-idle-connection-grace-period: "0s"
  tofqdns-max-deferred-connection-deletes: "10000"
  tofqdns-proxy-response-max-delay: "100ms"
  agent-not-ready-taint-key: "node.cilium.io/agent-not-ready"

  label-prefix-file: "/tmp/cilium/label-prefix/label-prefix"

  mesh-auth-enabled: "true"
  mesh-auth-queue-size: "1024"
  mesh-auth-rotated-identities-queue-size: "1024"
  mesh-auth-gc-interval: "5m0s"

  proxy-xff-num-trusted-hops-ingress: "0"
  proxy-xff-num-trusted-hops-egress: "0"
  proxy-connect-timeout: "2"
  proxy-initial-fetch-timeout: "30"
  proxy-max-requests-per-connection: "0"
  proxy-max-connection-duration-seconds: "0"
  proxy-idle-timeout-seconds: "60"
  proxy-max-concurrent-retries: "128"
  http-retry-count: "3"

  external-envoy-proxy: "true"
  envoy-base-id: "0"
  envoy-access-log-buffer-size: "4096"

  envoy-keep-cap-netbindservice: "false"

  clustermesh-enable-endpoint-sync: "false"
  clustermesh-enable-mcs-api: "false"

  nat-map-stats-entries: "32"
  nat-map-stats-interval: "30s"
  enable-internal-traffic-policy: "true"
  enable-lb-ipam: "true"
  enable-non-default-deny-policies: "true"
  enable-source-ip-verification: "true"


{{- if hasKey .Values "blacklistConflictingRoutes" }}
  # Configure blacklisting of local routes not owned by Cilium.
  blacklist-conflicting-routes: {{ .Values.blacklistConflictingRoutes | quote }}
{{- end }}
